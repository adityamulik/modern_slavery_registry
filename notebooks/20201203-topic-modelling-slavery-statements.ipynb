{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l7WeR-r6uf4x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import (CountVectorizer,\n",
    "                                             TfidfVectorizer)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modern_slavery_registry.utils import (sort_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = getcwd()\n",
    "DATA_PATH = DATA_PATH.replace(\"notebooks\", \"\")\n",
    "DATA_PATH += \"data\"\n",
    "SHEETS_PATH = DATA_PATH + \"\\\\sheets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "19-wl6Uw8GLu"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(f\"{SHEETS_PATH}\\\\subset_data.xlsx\")\n",
    "data.fillna(\"#NA\", inplace=True)\n",
    "data.columns = [\"URL\", \"final_statement_cleaned\", \"final_statement_cleaned_2\", \"duplicate_99per\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "jSU34WKL8ctQ",
    "outputId": "63b715fb-d36a-4718-9114-3a5b13b01375"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>final_statement_cleaned</th>\n",
       "      <th>final_statement_cleaned_2</th>\n",
       "      <th>duplicate_99per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://img1.wsimg.com/blobby/go/7695baff-3f0f...</td>\n",
       "      <td>holdinc europe ltd modern slavery act transpar...</td>\n",
       "      <td>holdinc europe ltd modern slavery act transpar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://1spatial.com/who-we-are/legal/modern-s...</td>\n",
       "      <td>independent research edison investments modern...</td>\n",
       "      <td>independent research edison investments modern...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.shazans.com/slavery-and-human-traf...</td>\n",
       "      <td>slavery human trafficking statement slavery hu...</td>\n",
       "      <td>slavery human traffic statement slavery human ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.business-humanrights.org/sites/def...</td>\n",
       "      <td>modern slavery atement modern slavery atement ...</td>\n",
       "      <td>modern slavery atement modern slavery atement ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.2agriculture.com/wp-content/upload...</td>\n",
       "      <td>modern slavery act slavery human trafficking s...</td>\n",
       "      <td>modern slavery act slavery human traffic state...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://img1.wsimg.com/blobby/go/7695baff-3f0f...   \n",
       "1  https://1spatial.com/who-we-are/legal/modern-s...   \n",
       "2  https://www.shazans.com/slavery-and-human-traf...   \n",
       "3  https://www.business-humanrights.org/sites/def...   \n",
       "4  https://www.2agriculture.com/wp-content/upload...   \n",
       "\n",
       "                             final_statement_cleaned  \\\n",
       "0  holdinc europe ltd modern slavery act transpar...   \n",
       "1  independent research edison investments modern...   \n",
       "2  slavery human trafficking statement slavery hu...   \n",
       "3  modern slavery atement modern slavery atement ...   \n",
       "4  modern slavery act slavery human trafficking s...   \n",
       "\n",
       "                           final_statement_cleaned_2  duplicate_99per  \n",
       "0  holdinc europe ltd modern slavery act transpar...            False  \n",
       "1  independent research edison investments modern...            False  \n",
       "2  slavery human traffic statement slavery human ...            False  \n",
       "3  modern slavery atement modern slavery atement ...            False  \n",
       "4  modern slavery act slavery human traffic state...            False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "66EntRIr8LwH"
   },
   "outputs": [],
   "source": [
    "N_GRAMS = (1, 1)\n",
    "count_vect = CountVectorizer(ngram_range=N_GRAMS)\n",
    "X = count_vect.fit_transform(data[\"final_statement_cleaned_2\"].values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4HELo1g8f-X",
    "outputId": "3b4240ee-1024-4052-8427-804f417dd7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_jobs=-1, random_state=40, verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TOPICS = 10\n",
    "LDA = LatentDirichletAllocation(n_components=N_TOPICS, n_jobs=-1, verbose=1, random_state=RANDOM_STATE)\n",
    "LDA.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['service', 'us', 'contact', 'support', 'care', 'news', 'new', 'solutions', 'home', 'people', 'career', 'products', 'energy', 'water', 'help']\n",
      "\n",
      "Topic 1: ['slavery', 'modern', 'traffic', 'human', 'supply', 'chain', 'business', 'ensure', 'statement', 'policy', 'act', 'risk', 'suppliers', 'take', 'within']\n",
      "\n",
      "Topic 2: ['use', 'term', 'may', 'site', 'information', 'shall', 'website', 'service', 'content', 'condition', 'include', 'right', 'order', 'time', 'without']\n",
      "\n",
      "Topic 3: ['human', 'suppliers', 'supply', 'conduct', 'chain', 'traffic', 'code', 'supplier', 'labor', 'slavery', 'business', 'compliance', 'include', 'audit', 'employees']\n",
      "\n",
      "Topic 4: ['www', 'https', 'com', 'uk', 'en', 'cking', 'class', 'store', 'div', 'co', 'tra', 'http', 'span', 'brand', 'shop']\n",
      "\n",
      "Topic 5: ['information', 'use', 'data', 'us', 'personal', 'cookies', 'may', 'website', 'site', 'service', 'privacy', 'please', 'provide', 'contact', 'process']\n",
      "\n",
      "Topic 6: ['service', 'firm', 'financial', 'include', 'risk', 'council', 'bank', 'investment', 'management', 'modern', 'slavery', 'business', 'provide', 'human', 'clients']\n",
      "\n",
      "Topic 7: ['slavery', 'human', 'business', 'suppliers', 'modern', 'risk', 'traffic', 'supply', 'chain', 'conduct', 'statement', 'include', 'code', 'company', 'supplier']\n",
      "\n",
      "Topic 8: ['group', 'company', 'limit', 'business', 'right', 'uk', 'human', 'work', 'ltd', 'employees', 'products', 'policy', 'statement', 'labour', 'suppliers']\n",
      "\n",
      "Topic 9: ['modern', 'slavery', 'risk', 'suppliers', 'work', 'supply', 'right', 'chain', 'labour', 'human', 'include', 'business', 'audit', 'train', 'ethical']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_TOP_WORDS = 15 \n",
    "feature_names = count_vect.get_feature_names()\n",
    "topic_components = LDA.components_ \n",
    "# Note: topic words are words with high LDA component values\n",
    "for i in range(N_TOPICS):\n",
    "    print(f\"Topic {i}: {[feature_names[j] for j in np.argsort(topic_components[i])[::-1][:N_TOP_WORDS]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicate statements (with 99% duplicacy) and topic modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_statements = data[data[\"duplicate_99per\"]==False][[\"URL\", \"final_statement_cleaned_2\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements before removing duplicate: 11967\n",
      "Number of statements after removing duplicate(99%): 10780\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of statements before removing duplicate: {len(data)}\")\n",
    "print(f\"Number of statements after removing duplicate(99%): {len(unique_statements)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Getting idea about vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10780it [00:05, 1951.02it/s]\n"
     ]
    }
   ],
   "source": [
    "term_freq = {} # to keep track of term frequency\n",
    "document_freq = {} # to keep track of document-term frequency\n",
    "last_doc = {}\n",
    "for i, statement in tqdm(enumerate(unique_statements[\"final_statement_cleaned_2\"]),\n",
    "                      position=0, \n",
    "                      leave=True):\n",
    "    for word in statement.split():  \n",
    "        if word not in term_freq:\n",
    "            term_freq[word] = 1\n",
    "            document_freq[word] = 1\n",
    "        else:\n",
    "            term_freq[word] += 1\n",
    "            if last_doc[word] != i:\n",
    "                document_freq[word] += 1\n",
    "        last_doc[word] = i\n",
    "        \n",
    "total_docs = len(unique_statements)\n",
    "document_freq = {k:v/total_docs for k,v in document_freq.items()} \n",
    "del last_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 52259\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(term_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DF = .7 # minimum document frequency\n",
    "MIN_DF = 2e-4 # maximum document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'slavery': 0.885899814471243,\n",
       " 'supply': 0.8664192949907236,\n",
       " 'human': 0.8648423005565863,\n",
       " 'business': 0.8611317254174397,\n",
       " 'chain': 0.8564935064935065,\n",
       " 'act': 0.8551948051948052,\n",
       " 'traffic': 0.8440630797773655,\n",
       " 'statement': 0.8154916512059369,\n",
       " 'suppliers': 0.8085343228200371,\n",
       " 'modern': 0.8077922077922078,\n",
       " 'ensure': 0.7928571428571428,\n",
       " 'include': 0.7880333951762524,\n",
       " 'risk': 0.7687384044526901,\n",
       " 'take': 0.7603896103896104,\n",
       " 'policy': 0.7221706864564007,\n",
       " 'make': 0.7188311688311688,\n",
       " 'company': 0.7157699443413729,\n",
       " 'commit': 0.7151205936920223,\n",
       " 'employees': 0.7086270871985157}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing below words\n",
    "sort_dict({k:v for k,v in document_freq.items() if v > MAX_DF}, by=1, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size before filtering based on document frequency  : 52259\n",
      "vocab size after filtering based on document frequency [ 0.00020, 0.70000]: 16480\n"
     ]
    }
   ],
   "source": [
    "filtered_document_freq = {k:v for k,v in document_freq.items() if MIN_DF <= v <= MAX_DF}\n",
    "print(f\"vocab size before filtering based on document frequency  : {len(document_freq)}\")\n",
    "print(f\"vocab size after filtering based on document frequency [{MIN_DF: .5f},{MAX_DF: .5f}]: {len(filtered_document_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "66EntRIr8LwH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectorizer matrix shape: (10780, 16480)\n"
     ]
    }
   ],
   "source": [
    "N_GRAMS = (1, 1)\n",
    "count_vect = CountVectorizer(ngram_range=N_GRAMS, min_df=MIN_DF, max_df=MAX_DF)\n",
    "X = count_vect.fit_transform(unique_statements[\"final_statement_cleaned_2\"].values) \n",
    "print(f\"Count vectorizer matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4HELo1g8f-X",
    "outputId": "3b4240ee-1024-4052-8427-804f417dd7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_jobs=-1, random_state=40, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_TOPICS = 10\n",
    "LDA = LatentDirichletAllocation(n_components=N_TOPICS,\n",
    "                                n_jobs=-1, \n",
    "                                verbose=1, \n",
    "                                random_state=RANDOM_STATE)\n",
    "LDA.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['group', 'limit', 'within', 'uk', 'service', 'place', 'policies', 'part', 'process', 'staff', 'year', 'financial', 'train', 'review', 'work']\n",
      "\n",
      "Topic 1: ['service', 'work', 'support', 'staff', 'council', 'trust', 'safeguard', 'people', 'care', 'uk', 'home', 'report', 'procurement', 'provide', 'new']\n",
      "\n",
      "Topic 2: ['use', 'information', 'data', 'us', 'may', 'website', 'service', 'personal', 'site', 'cookies', 'term', 'privacy', 'provide', 'contact', 'right']\n",
      "\n",
      "Topic 3: ['conduct', 'code', 'supplier', 'labor', 'compliance', 'audit', 'require', 'standards', 'laws', 'train', 'force', 'comply', 'applicable', 'right', 'provide']\n",
      "\n",
      "Topic 4: ['right', 'work', 'global', 'value', 'management', 'principles', 'people', 'support', 'conduct', 'service', 'respect', 'report', 'world', 'corporate', 'policies']\n",
      "\n",
      "Topic 5: ['group', 'right', 'conduct', 'code', 'labour', 'supplier', 'global', 'principles', 'uk', 'compliance', 'corporate', 'respect', 'sustainability', 'force', 'products']\n",
      "\n",
      "Topic 6: ['organisation', 'activities', 'workers', 'review', 'conduct', 'work', 'action', 'policies', 'supplier', 'train', 'code', 'use', 'labour', 'within', 'relate']\n",
      "\n",
      "Topic 7: ['supplier', 'train', 'work', 'service', 'process', 'group', 'uk', 'report', 'provide', 'labour', 'identify', 'within', 'review', 'assessment', 'right']\n",
      "\n",
      "Topic 8: ['work', 'labour', 'ethical', 'workers', 'right', 'audit', 'source', 'trade', 'issue', 'uk', 'train', 'practice', 'brand', 'products', 'base']\n",
      "\n",
      "Topic 9: ['www', 'https', 'com', 'work', 'breach', 'part', 'form', 'may', 'person', 'labour', 'responsibility', 'report', 'force', 'cking', 'us']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_TOP_WORDS = 15 \n",
    "feature_names = count_vect.get_feature_names()\n",
    "topic_components = LDA.components_ \n",
    "# Note: topic words are words with high LDA component values\n",
    "for i in range(N_TOPICS):\n",
    "    print(f\"Topic {i}: {[feature_names[j] for j in np.argsort(topic_components[i])[::-1][:N_TOP_WORDS]]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20201203-Topic-Modelling-Slavery-Statements.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "modern_slavery",
   "language": "python",
   "name": "modern_slavery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
