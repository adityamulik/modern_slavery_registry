{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if autocompletion is not working\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "l7WeR-r6uf4x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import (CountVectorizer)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from os import getcwd, path\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import json\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modern_slavery_registry.utils import dump_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data with final cleaned statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = getcwd().replace(\n",
    "    path.basename(getcwd()), \"\")\n",
    "DATA_PATH = path.join(PROJECT_PATH, \"data\")\n",
    "MODEL_SAVE_PATH = path.join(PROJECT_PATH, \"models\")\n",
    "SHEETS_PATH = path.join(DATA_PATH, \"sheets\")\n",
    "PICKLE_PATH = path.join(DATA_PATH, \"data\\\\pickles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "19-wl6Uw8GLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9993 non-NA statements\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(f\"{SHEETS_PATH}\\\\subset_data.xlsx\")\n",
    "data.fillna(\"#NA\", inplace=True)\n",
    "data = data[[\"URL\", \"final_statement_cleaned\"]]\n",
    "n_sentences = len(data)\n",
    "print(f\"Found {n_sentences} non-NA statements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "jSU34WKL8ctQ",
    "outputId": "63b715fb-d36a-4718-9114-3a5b13b01375"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>final_statement_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://img1.wsimg.com/blobby/go/7695baff-3f0f...</td>\n",
       "      <td>66 99 km sh foor eum hold europe ltd 200 alder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://1spatial.com/who-we-are/legal/modern-s...</td>\n",
       "      <td>modern slavery act policy statement home solut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.shazans.com/slavery-and-human-traf...</td>\n",
       "      <td>slavery human traffic statement shazans shazan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.business-humanrights.org/sites/def...</td>\n",
       "      <td>28 2019 modern slavery statement 2018 make pur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.2agriculture.com/wp-content/upload...</td>\n",
       "      <td>fh modern slavery act 2015 human traffic state...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://img1.wsimg.com/blobby/go/7695baff-3f0f...   \n",
       "1  https://1spatial.com/who-we-are/legal/modern-s...   \n",
       "2  https://www.shazans.com/slavery-and-human-traf...   \n",
       "3  https://www.business-humanrights.org/sites/def...   \n",
       "4  https://www.2agriculture.com/wp-content/upload...   \n",
       "\n",
       "                             final_statement_cleaned  \n",
       "0  66 99 km sh foor eum hold europe ltd 200 alder...  \n",
       "1  modern slavery act policy statement home solut...  \n",
       "2  slavery human traffic statement shazans shazan...  \n",
       "3  28 2019 modern slavery statement 2018 make pur...  \n",
       "4  fh modern slavery act 2015 human traffic state...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating NGRAMS explicitly, not using Gensim NGRAMS - works like a black-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88971c9ee434596841039d8b159d6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NGRAMS = (1, 2)\n",
    "ngrams_from_sentences = []\n",
    "\n",
    "for sentence in tqdm(data[\"final_statement_cleaned\"].values):\n",
    "    sentence = sentence.split()\n",
    "    ngrams_from_sentence = []\n",
    "    len_sentence = len(sentence)\n",
    "    for n in range(NGRAMS[0], NGRAMS[1]+1):\n",
    "        for i in range(len_sentence-n + 1):\n",
    "            ngrams_from_sentence.append(\" \".join(sentence[i:i+n]))\n",
    "    #     # preparing ngrams at end of sentence\n",
    "    #     for i in range(len_sentence-ngram+1, len_sentence):\n",
    "    #         ngram_sentence.append(\" \".join(\n",
    "    #             sentence[i:] + [\"$PAD$\"] * (ngram -  len(sentence[i :]))))\n",
    "    ngrams_from_sentences.append(ngrams_from_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['66', '99', 'km', 'sh', 'foor', 'eum', 'hold', 'europe', 'ltd', '200', 'aldersgate', 'street', 'london', 'ecia', '4hd', 'tel', '020', '7382', '6500', 'mail', 'keulongen', 'uk', 'kline', 'com', 'modern', 'slavery', 'act', 'transparency', 'statement', 'crane', 'lineure', 'eee', 'publish', '22', 'march', '2019', '2015', 'require', 'large', 'entity', 'carry', 'business', 'publish', 'detail', 'effort', 'combat', 'human', 'traffic', 'day', 'relate', 'action', 'activity', 'financial', 'year', 'april', '2018', '31', 'part', 'ship', 'industry', 'group', 'recognize', 'responsibility', 'take', 'robust', 'approach', 'absolutely', 'commit', 'prevent', 'corporate', 'ensure', 'supply', 'chain', 'free', 'organizational', 'structure', 'global', 'entity', 'headquarter', 'tokyo', 'japan', 'network', 'office', 'around', 'globe', 'include', 'united', 'kingdom', 'comprise', 'bulk', 'lng', 'polar', 'unit', 'include', 'car', 'carrier', 'dry', 'management', 'operation', 'train', 'relevant', 'policy', 'charter', 'conduct', 'company', 'adhere', 'emphasizes', 'due', 'respect', 'right', 'compliance', 'applicable', 'law', 'ordinance', 'rule', 'abides', 'labor', 'regulation', 'tolerate', 'child', 'force', 'present', 'new', 'staff', 'full', 'view', 'whistleblowing', 'policy', 'organization', 'encourage', 'worker', 'customer', 'partner', 'report', 'concern', 'relate', 'direct', 'include', 'circumstance', 'may', 'give', 'rise', 'enhance', 'risk', 'whistieblowing', 'procedure', 'design', 'make', 'easy', 'disclosure', 'without', 'fear', 'retaliation', 'diligence', 'manage', 'part', 'chain', 'carry', 'inherent', 'risk', 'adverse', 'social', 'environmental', 'impact', 'less', 'oversight', 'control', 'work', 'condition', 'recruit', 'practice', 'would', 'ship', 'construction', 'dismantle', 'employment', 'subcontract', 'could', 'present', 'use', 'third', 'party', 'man', 'crew', 'agency', 'business', 'situation', 'site', 'visit', 'audit', 'type', 'process', 'formalize', 'assessment', 'employ', 'addition', 'directly', 'manage', 'relatively', 'sophisticated', 'sector', 'exposed', 'unsatisfactory', 'also', 'develop', 'guideline', 'detail', 'requirement', 'supplier', 'adherence', 'abolition', 'find', 'df', 'register', 'office', '6th', 'fioor', 'ec1a', 'company', 'england', 'ne', '5005018', 'awareness', 'raise', 'program', 'raise', 'issue', 'put', 'poster', 'across', 'premise', 'explain', 'basic', 'principle', 'external', 'help', 'available', 'example', 'helpline', 'note', 'transportation', 'must', 'already', 'ensure', 'vessel', 'use', 'illegal', 'purpose', 'people', 'engage', 'contractor', 'procedure', 'place', 'protect', 'activity', 'mr', 'akira', 'misaki', 'director', '66 99', '99 km', 'km sh', 'sh foor', 'foor eum', 'eum hold', 'hold europe', 'europe ltd', 'ltd 200', '200 aldersgate', 'aldersgate street', 'street london', 'london ecia', 'ecia 4hd', '4hd tel', 'tel 020', '020 7382', '7382 6500', '6500 mail', 'mail keulongen', 'keulongen uk', 'uk kline', 'kline com', 'com modern', 'modern slavery', 'slavery act', 'act transparency', 'transparency statement', 'statement crane', 'crane lineure', 'lineure eee', 'eee publish', 'publish 22', '22 march', 'march 2019', '2019 2015', '2015 require', 'require large', 'large entity', 'entity carry', 'carry business', 'business publish', 'publish detail', 'detail effort', 'effort combat', 'combat human', 'human traffic', 'traffic day', 'day relate', 'relate action', 'action activity', 'activity financial', 'financial year', 'year april', 'april 2018', '2018 31', '31 part', 'part ship', 'ship industry', 'industry group', 'group recognize', 'recognize responsibility', 'responsibility take', 'take robust', 'robust approach', 'approach absolutely', 'absolutely commit', 'commit prevent', 'prevent corporate', 'corporate ensure', 'ensure supply', 'supply chain', 'chain free', 'free organizational', 'organizational structure', 'structure global', 'global entity', 'entity headquarter', 'headquarter tokyo', 'tokyo japan', 'japan network', 'network office', 'office around', 'around globe', 'globe include', 'include united', 'united kingdom', 'kingdom comprise', 'comprise bulk', 'bulk lng', 'lng polar', 'polar unit', 'unit include', 'include car', 'car carrier', 'carrier dry', 'dry management', 'management operation', 'operation train', 'train relevant', 'relevant policy', 'policy charter', 'charter conduct', 'conduct company', 'company adhere', 'adhere emphasizes', 'emphasizes due', 'due respect', 'respect right', 'right compliance', 'compliance applicable', 'applicable law', 'law ordinance', 'ordinance rule', 'rule abides', 'abides labor', 'labor regulation', 'regulation tolerate', 'tolerate child', 'child force', 'force present', 'present new', 'new staff', 'staff full', 'full view', 'view whistleblowing', 'whistleblowing policy', 'policy organization', 'organization encourage', 'encourage worker', 'worker customer', 'customer partner', 'partner report', 'report concern', 'concern relate', 'relate direct', 'direct include', 'include circumstance', 'circumstance may', 'may give', 'give rise', 'rise enhance', 'enhance risk', 'risk whistieblowing', 'whistieblowing procedure', 'procedure design', 'design make', 'make easy', 'easy disclosure', 'disclosure without', 'without fear', 'fear retaliation', 'retaliation diligence', 'diligence manage', 'manage part', 'part chain', 'chain carry', 'carry inherent', 'inherent risk', 'risk adverse', 'adverse social', 'social environmental', 'environmental impact', 'impact less', 'less oversight', 'oversight control', 'control work', 'work condition', 'condition recruit', 'recruit practice', 'practice would', 'would ship', 'ship construction', 'construction dismantle', 'dismantle employment', 'employment subcontract', 'subcontract could', 'could present', 'present use', 'use third', 'third party', 'party man', 'man crew', 'crew agency', 'agency business', 'business situation', 'situation site', 'site visit', 'visit audit', 'audit type', 'type process', 'process formalize', 'formalize assessment', 'assessment employ', 'employ addition', 'addition directly', 'directly manage', 'manage relatively', 'relatively sophisticated', 'sophisticated sector', 'sector exposed', 'exposed unsatisfactory', 'unsatisfactory also', 'also develop', 'develop guideline', 'guideline detail', 'detail requirement', 'requirement supplier', 'supplier adherence', 'adherence abolition', 'abolition find', 'find df', 'df register', 'register office', 'office 6th', '6th fioor', 'fioor ec1a', 'ec1a company', 'company england', 'england ne', 'ne 5005018', '5005018 awareness', 'awareness raise', 'raise program', 'program raise', 'raise issue', 'issue put', 'put poster', 'poster across', 'across premise', 'premise explain', 'explain basic', 'basic principle', 'principle external', 'external help', 'help available', 'available example', 'example helpline', 'helpline note', 'note transportation', 'transportation must']\n"
     ]
    }
   ],
   "source": [
    "print(ngrams_from_sentences[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating vocab from `NGRAMS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50361071f964eaf8938257b29a21b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram_term_freq = {} # to keep track of term frequency\n",
    "ngram_document_freq = {} # to keep track of document-term frequency\n",
    "ngram_last_doc = {}\n",
    "for i, ngrams_from_sentence in tqdm(enumerate(ngrams_from_sentences)):\n",
    "    for ngram in ngrams_from_sentence:  \n",
    "        if ngram not in ngram_term_freq:\n",
    "            ngram_term_freq[ngram] = 1\n",
    "            ngram_document_freq[ngram] = 1\n",
    "        else:\n",
    "            ngram_term_freq[ngram] += 1\n",
    "            if ngram_last_doc[ngram] != i:\n",
    "                ngram_document_freq[ngram] += 1\n",
    "        ngram_last_doc[ngram] = i\n",
    "        \n",
    "ngram_document_freq = {ngram: freq/n_sentences for ngram, freq in ngram_document_freq.items()} \n",
    "del ngram_last_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1265192\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(ngram_term_freq)}\") # without padding last ngrams word in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.265192e+06</td>\n",
       "      <td>1.265192e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.102946e+00</td>\n",
       "      <td>4.854976e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.731479e+01</td>\n",
       "      <td>7.970769e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000700e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000700e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000700e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.001401e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.850300e+04</td>\n",
       "      <td>9.589713e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term_freq      doc_freq\n",
       "count  1.265192e+06  1.265192e+06\n",
       "mean   5.102946e+00  4.854976e-04\n",
       "std    9.731479e+01  7.970769e-03\n",
       "min    1.000000e+00  1.000700e-04\n",
       "25%    1.000000e+00  1.000700e-04\n",
       "50%    1.000000e+00  1.000700e-04\n",
       "75%    2.000000e+00  2.001401e-04\n",
       "max    1.850300e+04  9.589713e-01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_stat_table = pd.DataFrame({\"ngram\": ngram_term_freq.keys(), \n",
    "                                 \"term_freq\": ngram_term_freq.values(),\n",
    "                                 \"doc_freq\": ngram_document_freq.values()})\n",
    "ngram_stat_table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting vocab of interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "66EntRIr8LwH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)-grams vocab size with doc frequency ( 0.001,  0.100): 48507\n",
      "(1, 2)-grams vocab size with doc frequency ( 0.001,  0.100): 3.834 %\n"
     ]
    }
   ],
   "source": [
    "MIN_DF = 10/n_sentences # ngrams present in atleast 10 docs out of total\n",
    "MAX_DF = 1000/n_sentences  # consider ngrams present in atmost 1000 docs out of total\n",
    "ngram_covered = len(ngram_stat_table[ngram_stat_table[\"doc_freq\"].between(MIN_DF, MAX_DF)])\n",
    "print(f\"{NGRAMS}-grams vocab size with doc frequency ({MIN_DF: .3f}, {MAX_DF: .3f}): \"\n",
    "      f\"{ngram_covered}\")\n",
    "print(f\"{NGRAMS}-grams vocab size with doc frequency ({MIN_DF: .3f}, {MAX_DF: .3f}): \"\n",
    "      f\"{ngram_covered*100/len(ngram_document_freq):.3f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for Gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9993, 48507)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=NGRAMS, min_df=MIN_DF, max_df=MAX_DF)\n",
    "X = count_vect.fit_transform(data[\"final_statement_cleaned\"].values) \n",
    "print(f\"shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d80e95d9ed445718d0ebc7994e23259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word2idx = count_vect.vocabulary_\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "data_for_model = []\n",
    "for row in tqdm(X.toarray()):\n",
    "    idxs = np.where(row > 0)\n",
    "    data_for_model.append([(idx, row[idx]) for idx in idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = (2, 201)\n",
    "LDA_MODELS = {}\n",
    "model_name_template = \"sklearn_bigrams_gensim_lda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c9df86aa2f42e9980f07bd886253a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5h 20min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build LDA model\n",
    "\n",
    "for n in tqdm(range(*N_TOPICS)):\n",
    "    LDA_MODELS[n] = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=data_for_model,\n",
    "        id2word=idx2word, \n",
    "        num_topics=n,\n",
    "        update_every=1,\n",
    "        chunksize=1000,\n",
    "        passes=1,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True,\n",
    "        iterations=50,\n",
    "        random_state=RANDOM_STATE)\n",
    "    dump_pickle(\n",
    "        obj=LDA_MODELS[n], filename=f\"{model_name_template}_{n}_topics\", path=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_keywords(\n",
    "    lda_model: gensim.models.ldamodel.LdaModel,\n",
    "    num_words: int=10,\n",
    "    num_topics: int=-1, \n",
    "    print_: bool=False)->Dict[int, str]:\n",
    "    topics_dict = {}\n",
    "    for topic in lda_model.print_topics(num_words=num_words, num_topics=num_topics):\n",
    "        topics_dict[topic[0]] = topic[1]\n",
    "        if print_:\n",
    "            print(f\"{topic[0]}: {topic[1:]}\")\n",
    "            print()\n",
    "            \n",
    "    return topics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8228d496ae524695b1774d8f8e5d4c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topics = {n_topic: print_topic_keywords(LDA_MODELS[n_topic]) for n_topic in tqdm(range(*N_TOPICS))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join(DATA_PATH, f\"{model_name_template}_topics.json\"), \"w\") as f:\n",
    "    json.dump(topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ('0.003*\"chain act\" + 0.003*\"california transparency\" + 0.003*\"transparency supply\" + 0.002*\"retaliation\" + 0.002*\"maintain high\" + 0.002*\"circumstance\" + 0.002*\"put place\" + 0.002*\"fear retaliation\" + 0.002*\"aim ensure\" + 0.002*\"specify\"',)\n",
      "\n",
      "1: ('0.002*\"arrangement\" + 0.002*\"regulate\" + 0.001*\"annual turnover\" + 0.001*\"modern act\" + 0.001*\"wholly\" + 0.001*\"54 constitute\" + 0.001*\"clause\" + 0.001*\"software\" + 0.001*\"constitute financial\" + 0.001*\"ultimate parent\"',)\n",
      "\n",
      "2: ('0.004*\"exploit\" + 0.004*\"violation fundamental\" + 0.004*\"various form\" + 0.004*\"liberty\" + 0.004*\"person liberty\" + 0.003*\"commercial gain\" + 0.003*\"deprivation\" + 0.003*\"crime violation\" + 0.003*\"liberty another\" + 0.003*\"knowingly\"',)\n",
      "\n",
      "3: ('0.002*\"compact\" + 0.002*\"un\" + 0.002*\"ilo\" + 0.002*\"convention\" + 0.002*\"component\" + 0.002*\"freedom association\" + 0.002*\"universal\" + 0.001*\"grievance\" + 0.001*\"continuous\" + 0.001*\"goal\"',)\n",
      "\n",
      "4: ('0.004*\"shop\" + 0.004*\"vehicle\" + 0.003*\"car\" + 0.003*\"store\" + 0.003*\"accessory\" + 0.002*\"save\" + 0.002*\"gift\" + 0.002*\"book\" + 0.002*\"repair\" + 0.002*\"collection\"',)\n",
      "\n",
      "5: ('0.003*\"whistle blower\" + 0.003*\"level understand\" + 0.003*\"initiative identify\" + 0.003*\"anti policy\" + 0.003*\"commit act\" + 0.003*\"place within\" + 0.003*\"high level\" + 0.002*\"eligibility\" + 0.002*\"within business\" + 0.002*\"understand risk\"',)\n",
      "\n",
      "6: ('0.002*\"send\" + 0.002*\"collect\" + 0.002*\"user\" + 0.002*\"interest\" + 0.002*\"notice\" + 0.002*\"return\" + 0.002*\"post\" + 0.002*\"extent\" + 0.001*\"store\" + 0.001*\"damage\"',)\n",
      "\n",
      "7: ('0.002*\"handbook\" + 0.002*\"modern act\" + 0.001*\"2015 constitute\" + 0.001*\"policy procedure\" + 0.001*\"minimize\" + 0.001*\"equal opportunity\" + 0.001*\"endeavor\" + 0.001*\"msa\" + 0.001*\"executive officer\" + 0.001*\"bribery corruption\"',)\n",
      "\n",
      "8: ('0.002*\"investor\" + 0.002*\"france\" + 0.002*\"award\" + 0.002*\"ireland\" + 0.002*\"germany\" + 0.001*\"australia\" + 0.001*\"energy\" + 0.001*\"spain\" + 0.001*\"india\" + 0.001*\"africa\"',)\n",
      "\n",
      "9: ('0.002*\"tender\" + 0.002*\"embed\" + 0.001*\"spend\" + 0.001*\"victim\" + 0.001*\"strengthen\" + 0.001*\"charity\" + 0.001*\"equality\" + 0.001*\"roll\" + 0.001*\"council\" + 0.001*\"school\"',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = print_topic_keywords(LDA_MODELS[10], print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing log perpexity w.r.t. to number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3f44a771b34c42a0ba5355b3c373fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOG_PERPLEXITIES = {}\n",
    "for n in tqdm(range(*N_TOPICS)):\n",
    "    if n not in LOG_PERPLEXITIES.keys():\n",
    "        LOG_PERPLEXITIES[n] = LDA_MODELS[n].log_perplexity(corpus)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20201203-Topic-Modelling-Slavery-Statements.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "modern_slavery",
   "language": "python",
   "name": "modern_slavery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
