---
title: "Test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls(all=T))
# uncomment this if running first time on local
# To rum on AWS, always uncomment this
# install.packages(c( "tidyverse", "readxl", "tm", "topicmodels", "NLP"))
```

```{r}

library(tm)
library(topicmodels)
library(NLP)
library(readxl)
# options(stringsAsFactors = F) 
```

# Loading statements
```{r}
docs <- readxl::read_excel("E:/Projects/modern_slavery_registry/data/sheets/subset_data.xlsx")
docs <- docs[["final_statement_cleaned"]]
# https://stackoverflow.com/questions/47406555/error-faced-while-using-tm-packages-vcorpus-in-r
docs <- data.frame(doc_id=c(1:length(docs)), text=docs)
```

# Preparing document-term matrix for n-grams [1-n] 
```{r}
# https://stackoverflow.com/questions/45697840/custom-tokenizer-in-tm-package-r-not-working
corpus <- VCorpus(DataframeSource(docs))

# https://stackoverflow.com/questions/52207021/r-how-to-apply-terms-from-training-document-term-matrix-dtm-to-test-dtm-bot
custom_tokenizer <- function(x) unlist(lapply(NLP::ngrams(words(x), 1:2), paste, collapse = " "), use.names = FALSE)


MIN_DF <- 10
MAX_DF <- 1000
dtm <- DocumentTermMatrix(
  corpus, 
  control = list(
    tokenize=custom_tokenizer, 
    bounds = list(global = c(MIN_DF, MAX_DF))))
```

# Document-term dimension
```{r}
cat("docs:", dim(dtm)[1], "vocab-size:", dim(dtm)[2])
```
# Pre-process document-term matrix
```{r}
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(dtm) > 0
dtm <- dtm[sel_idx, ]
```

# Running LDA model
```{r}
# number of topics
NUM_TOPICS <- 10
# set random number generator seed
set.seed(40)
NUM_ITER = 500
# compute the LDA model, inference via 500 iterations of Gibbs sampling
topicModel <- LDA(
  x=dtm,
  k=NUM_TOPICS, 
  method="Gibbs", 
  control=list(iter = NUM_ITER, verbose = 25))

```
# Getting top n words for each topic
```{r}
NUM_TOP_WORDS = 10
terms(topicModel, NUM_TOP_WORDS)
```

```{r}
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
```

```{r}
# topics are probability distribtions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms 
```

```{r}
# aggregated beta distribution
rowSums(beta)  
```

# Computing perplexity for trained model
```{r}
perplexity(topicModel, dtm)
```